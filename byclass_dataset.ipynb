{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval\n",
        "!pip install torch_optimizer\n",
        "!pip install torchmetrics\n",
        "!pip install torchvision\n",
        "!pip install lion-pytorch\n",
        "!pip install timm\n",
        "\n"
      ],
      "metadata": {
        "id": "z_gcF9F6XUzx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CHCJ3mnG9ik",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from lion_pytorch import Lion\n",
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingWarmRestarts, SequentialLR\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchmetrics import F1Score\n",
        "from timm.data.mixup import Mixup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HISTEq:\n",
        "    def __call__(self, img):\n",
        "      img_np = np.array(img)\n",
        "      if img_np.dtype != np.uint8:\n",
        "         img_np = (img_np * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "      if len(img_np.shape) == 2:\n",
        "        equalized = cv2.equalizeHist(img_np)\n",
        "      else:\n",
        "         raise ValueError(\"Expected single-channel image\")\n",
        "\n",
        "      return transforms.functional.to_pil_image(equalized)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2VO249IPP9cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwzR_BbidGe8"
      },
      "outputs": [],
      "source": [
        "label_counts = {\n",
        "    1: 38374, 7: 35754, 3: 35143, 0: 34585, 6: 34232, 2: 34203, 8: 33946, 9: 33847, 4: 33535, 5: 31416,\n",
        "    24: 24983, 40: 24631, 28: 20764, 55: 18262, 47: 15318, 53: 14105, 30: 12602, 18: 11946, 49: 11418,\n",
        "    39: 10177, 12: 10094, 36: 10033, 29: 9820, 15: 9182, 22: 9002, 43: 8738, 25: 8347, 23: 8237, 10: 6407,\n",
        "    37: 5159, 21: 5076, 27: 5073, 14: 4934, 34: 4743, 32: 4695, 31: 4637, 13: 4562, 11: 3878, 19: 3762,\n",
        "    42: 3687, 17: 3152, 52: 2994, 57: 2910, 38: 2854, 56: 2830, 59: 2822, 33: 2771, 50: 2749, 61: 2725,\n",
        "    44: 2725, 35: 2701, 54: 2699, 58: 2697, 48: 2645, 26: 2605, 41: 2561, 16: 2517, 46: 2491, 20: 2468,\n",
        "    51: 2448, 60: 2365, 45: 1896\n",
        "}\n",
        "\n",
        "def compute_class_weights(label_counts):\n",
        "    total_sample = sum(label_counts.values())\n",
        "    num_classes = len(label_counts)\n",
        "\n",
        "\n",
        "    class_weights = {cls: total_sample / (num_classes * count) for cls, count in label_counts.items()}\n",
        "\n",
        "\n",
        "    max_weight = max(class_weights.values())\n",
        "    min_weight = min(class_weights.values())\n",
        "\n",
        "    class_weights = {cls: 1 + ((weight - min_weight) / (max_weight - min_weight)) for cls, weight in class_weights.items()}\n",
        "\n",
        "    sorted_classes = sorted(label_counts.keys())\n",
        "    class_weights_list = [class_weights[cls] for cls in sorted_classes]\n",
        "\n",
        "    return torch.FloatTensor(class_weights_list).to(device)\n",
        "\n",
        "\n",
        "class_weights_tensor = compute_class_weights(label_counts)\n",
        "\n",
        "print(\"Class Weights Tensor:\", class_weights_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform_for_Norm = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    HISTEq(),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "dataset = datasets.EMNIST(\n",
        "    root=\"data\",\n",
        "    split=\"byclass\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_for_Norm\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    num_workers=2,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "mean_sum    = 0.0\n",
        "sq_mean_sum = 0.0\n",
        "num_pixels  = 0\n",
        "\n",
        "for images, _ in loader:\n",
        "\n",
        "    images = images.to(torch.float32)  # ensure float\n",
        "\n",
        "\n",
        "    mean_sum    += images.sum(dim=[0, 2, 3])\n",
        "    sq_mean_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "    num_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "\n",
        "mean = mean_sum / num_pixels\n",
        "var  = (sq_mean_sum / num_pixels) - (mean ** 2)\n",
        "std  = torch.sqrt(var)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Std :\", std)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MJN0gFMd543s",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mean_value = mean\n",
        "std_value  = std\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    HISTEq(),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(p=0.1),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomAffine(degrees=2, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean_value, std=std_value)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    HISTEq(),\n",
        "    transforms.Resize((112, 112)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean_value, std=std_value)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fAi-YAtNqyBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loaders():\n",
        "\n",
        "\n",
        "  train_dataset = datasets.EMNIST(root=\"data\", split=\"byclass\", train=True, transform=train_transform, download=True)\n",
        "  test_dataset = datasets.EMNIST(root=\"data\", split=\"byclass\", train=False, transform= test_transform, download=True)\n",
        "\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=10,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        prefetch_factor=6\n",
        "    )\n",
        "  test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=512,\n",
        "        shuffle=False,\n",
        "        num_workers=10,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        prefetch_factor=6\n",
        "    )\n",
        "\n",
        "  return train_loader,test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "I4rUOWtZ5dnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class EMNISTModelB3(nn.Module):\n",
        "    def __init__(self, num_classes=62):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = models.efficientnet_b3(pretrained=True)\n",
        "\n",
        "        self.model.features[0][0] = nn.Conv2d(1, 40, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(1536, 768),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(768, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7kchAR5BBuK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, mixup_fn, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    batch_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (data, target) in enumerate(batch_iterator):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data, target = mixup_fn(data, target)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(data)\n",
        "\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "            loss = criterion(log_probs, target)\n",
        "\n",
        "        # Gradient accumulation\n",
        "        loss = loss / CONFIG['accumulation_steps']\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % CONFIG['accumulation_steps'] == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        total_loss += loss.item() * CONFIG['accumulation_steps']\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        _, target_hard = torch.max(target, 1)\n",
        "        correct += (predicted == target_hard).sum().item()\n",
        "        total += target_hard.size(0)\n",
        "\n",
        "        batch_iterator.set_postfix({\n",
        "            \"loss\": loss.item() * CONFIG['accumulation_steps'],\n",
        "            \"acc\": correct / total\n",
        "        })\n",
        "\n",
        "    return total_loss / len(train_loader), correct / total\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjdDaYssVz7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, test_loader, criterion, f1_metric):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            targets_one_hot = F.one_hot(targets, num_classes=62).type(torch.float32)\n",
        "            loss = criterion(F.log_softmax(outputs, dim=1), targets_one_hot)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            f1_metric.update(predicted, targets)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    f1 = f1_metric.compute().item()\n",
        "    f1_metric.reset()\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total, f1"
      ],
      "metadata": {
        "id": "JuTF8sEaMzqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "\n",
        "    'epochs': 15,\n",
        "    'accumulation_steps': 2\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "gwzmidZINdHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    model = EMNISTModelB3(num_classes=62).to(device)\n",
        "\n",
        "\n",
        "    train_loader, test_loader = data_loaders()\n",
        "\n",
        "\n",
        "    mixup_fn = Mixup(\n",
        "        mixup_alpha=0.22,\n",
        "        cutmix_alpha=0.62,\n",
        "        prob=0.8,\n",
        "        switch_prob=0.5,\n",
        "        mode='batch',\n",
        "        label_smoothing=0.1,\n",
        "        num_classes= 62\n",
        "    )\n",
        "\n",
        "    warmup_epochs = 5\n",
        "\n",
        "    f1_metric = F1Score(task=\"multiclass\", num_classes= 62, average='weighted').to(device)\n",
        "    optimizer = Lion(model.parameters(), lr=0.0003, weight_decay=0.01)\n",
        "\n",
        "    warmup_lambda = lambda epoch: float(epoch + 1) / warmup_epochs\n",
        "    warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
        "\n",
        "    cosine_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=8, T_mult=1, eta_min=1e-6)\n",
        "\n",
        "    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
        "    criterion = nn.KLDivLoss(weight = class_weights_tensor,reduction='batchmean')\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    best_f1 = 0\n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer,\n",
        "            scheduler, scaler, mixup_fn, epoch\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, f1 = validate(model, test_loader, criterion, f1_metric)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, \"\n",
        "              f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JC10-FJHOEGP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}